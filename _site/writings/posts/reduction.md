# generative reduction 

David Lewis (one might remember him as the "Holes" guy: [here](https://philpapers.org/rec/LEWH)), according to his Princeton biographical sketch, was known as "a materialist and a reductionist, arguing that states of the mind are simply states of the brain, functionally conceived." While I would agree with this view, I find it bothersome that a connection between materialism and reductionism is nearly automatic. 

When we think of reducing macroscopic phenomena to their constituent components, we conjure a simplification. "If only I could formalize the behaviors of the constituent components," we think, "I could understand how their interactions generate macroscopic phenomena." This is simplification, in my view, not reduction. 

For this reason, I call for a generative reductionism. Some might not see the difference between this and emergence (or emergentism, if you pray to it as some do). The difference lies in the amount that is swept under the rug. Formalizing the behavior of constituent components of a system is the good practice of science. With this I have no problem. However, expecting, by describing the patterns of human behavior, to understand politics is malarkey. There is micro and macro economics for a reason, you know. 

What I think we have to think harder about is understanding generation from local interactions. We can simulate automata and other local behavior and fine-tune to create patterns, sure, but can we understand how these patterns themselves form constituent components of a larger system? Even working towards two levels of hierarchy would be tremendous progress. That is, beginning with an encoding for a simple particle and ending with the interactions between groups of these particles, course-graining if you like, would be progress. 

I am not sure how to do this, or even how to measure success. I'm not sure if this is a simulation problem or a theoretical endeavor. My main point is that by reducing macroscopic phenomena to the local interactions and choices of its constituents, we are only doing half the job of a new scientific paradigm. The other half is generative: allow these constituents to interact, and find how they form patterns, structures, and constituents of a larger system. It is this (minimally) third-level macroscopic vision that we want to see. Once three levels can be simulated and described, we can find interactions between levels that are not neighbors in the hierarchy. 

Lastly, one might propose a network-of-networks model. In this case, how might be track the information of each subnetwork? If a typical network holds a value on each of its nodes and edges, how could we possibly reduce the activity of a single network to a scalar value and get away with it? A vector? A matrix? That's reduction, but we still generate-- the number of reduced scalars might even go above the number of elements within a single constituent network on the level below. Following this series of compressions and expansions and feedback between levels is a new kind of thinking that is daunting but bound to be fruitful. 

Worries include: does science allow for this kind of lateral thinking? Is this pseudoscience? How is this possible testable or even simulatable? 
