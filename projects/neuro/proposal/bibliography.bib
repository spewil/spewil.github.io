@article{Gallese2003,
abstract = {Starting from a neurobiological standpoint, I will propose that our capacity to understand others as intentional agents, far from being exclusively dependent upon mentalistic/linguistic abilities, be deeply grounded in the relational nature of our interactions with the world. According to this hypothesis, an implicit, prereflexive form of understanding of other individuals is based on the strong sense of identity binding us to them. We share with our conspecifics a multiplicity of states that include actions, sensations and emotions. A new conceptual tool able to capture the richness of the experiences we share with others will be introduced: the shared manifold of intersubjectivity. I will posit that it is through this shared manifold that it is possible for us to recognize other human beings as similar to us. It is just because of this shared manifold that intersubjective communication and ascription of intentionality become possible. It will be argued that the same neural structures that are involved in processing and controlling executed actions, felt sensations and emotions are also active when the same actions, sensations and emotions are to be detected in others. It therefore appears that a whole range of different "mirror matching mechanisms" may be present in our brain. This matching mechanism, constituted by mirror neurons originally discovered and described in the domain of action, could well be a basic organizational feature of our brain, enabling our rich and diversified intersubjective experiences. This perspective is in a position to offer a global approach to the understanding of the vulnerability to major psychoses such as schizophrenia.},
author = {Gallese, Vittorio},
doi = {10.1159/000072786},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/neural manifold/empathy{\_}shared{\_}manifold{\_}2003.pdf:pdf},
isbn = {0254-4962 (Print)$\backslash$r0254-4962 (Linking)},
issn = {02544962},
journal = {Psychopathology},
keywords = {Autism,Empathy,Intersubjectivity,Mirror neurons,Phenomenology,Schizophrenia,Shared manifold hypothesis},
number = {4},
pages = {171--180},
pmid = {14504450},
title = {{The roots of empathy: The shared manifold hypothesis and the neural basis of intersubjectivity}},
volume = {36},
year = {2003}
}
@article{Wickersham2007,
author = {Wickersham, Ian R and Finke, Stefan and Conzelmann, Karl-klaus and Callaway, Edward M},
doi = {10.1038/NMETH999},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/methods/rabies{\_}tracing{\_}2007.pdf:pdf},
number = {1},
pages = {47--49},
title = {{Retrograde neuronal tracing with a deletion-mutant rabies virus}},
volume = {4},
year = {2007}
}
@article{Gashler2008,
author = {Gashler, Michael S and Ventura, Dan and Martinez, Tony},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/methods/gashler2007nips.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst. 20},
keywords = {manifold learning},
pages = {513--520},
title = {{Iterative Non-linear Dimensionality Reduction with Manifold Sculpting}},
year = {2008}
}
@book{Schiff2012,
abstract = {Over the past sixty years, powerful methods of model-based control engineering have been responsible for such dramatic advances in engineering systems as autolanding aircraft, autonomous vehicles, and even weather forecasting. Over those same decades, our models of the nervous system have evolved from single-cell membranes to neuronal networks to large-scale models of the human brain. Yet until recently control theory was completely inapplicable to the types of nonlinear models being developed in neuroscience. The revolution in nonlinear control engineering in the late 1990s has made the intersection of control theory and neuroscience possible. In Neural Control Engineering, Steven Schiff seeks to bridge the two fields, examining the application of new methods in nonlinear control engineering to neuroscience. After presenting extensive material on formulating computational neuroscience models in a control environment -- including some fundamentals of the algorithms helpful in crossing the divide from intuition to effective application -- Schiff examines a range of applications, including brain-machine interfaces and neural stimulation. He reports on research that he and his colleagues have undertaken showing that nonlinear control theory methods can be applied to models of single cells, small neuronal networks, and large-scale networks in disease states of Parkinson's disease and epilepsy. With Neural Control Engineering the reader acquires a working knowledge of the fundamentals of control theory and computational neuroscience sufficient not only to understand the literature in this trandisciplinary area but also to begin working to advance the field. The book will serve as an essential guide for scientists in either biology or engineering and for physicians who wish to gain expertise in these areas.},
address = {Cambridge},
author = {Schiff, Steven J.},
doi = {10.1063/PT.3.1824},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/books/Schiff{\_}Neural{\_}Control.pdf:pdf},
isbn = {0262015374},
issn = {00319228},
pages = {361},
publisher = {MIT Press},
title = {{Neural Control Engineering: The Emerging Intersection Between Control Theory and Neuroscience}},
url = {https://books.google.com/books?id=P9UvTQtnqKwC{\&}pgis=1},
year = {2012}
}
@article{Orlandi2014,
abstract = {Neuronal dynamics are fundamentally constrained by the underlying structural network architecture, yet much of the details of this synaptic connectivity are still unknown even in neuronal cultures in vitro. Here we extend a previous approach based on information theory, the Generalized Transfer Entropy, to the reconstruction of connectivity of simulated neuronal networks of both excitatory and inhibitory neurons. We show that, due to the model-free nature of the developed measure, both kinds of connections can be reliably inferred if the average firing rate between synchronous burst events exceeds a small minimum frequency. Furthermore, we suggest, based on systematic simulations, that even lower spontaneous inter- burst rates could be raised to meet the requirements of our reconstruction algorithm by applying a weak spatially homogeneous stimulation to the entire network. By combining multiple recordings of the same in silico network before and after pharmacologically blocking inhibitory synaptic transmission, we show then how it becomes possible to infer with high confidence the excitatory or inhibitory nature of each individual neuron.},
archivePrefix = {arXiv},
arxivId = {1309.4287},
author = {Orlandi, Javier G. and Stetter, Olav and Soriano, Jordi and Geisel, Theo and Battaglia, Demian},
doi = {10.1371/journal.pone.0098842},
eprint = {1309.4287},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/methods/transfer{\_}entropy.PDF:PDF},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS One},
number = {6},
pmid = {24905689},
title = {{Transfer entropy reconstruction and labeling of neuronal connections from simulated calcium imaging}},
volume = {9},
year = {2014}
}
@article{Sadtler2014,
abstract = {Learning, whether motor, sensory or cognitive, requires networks of neurons to generate new activity patterns. As some behaviours are easier to learn than others, we asked if some neural activity patterns are easier to generate than others. Here we investigate whether an existing network constrains the patterns that a subset of its neurons is capable of exhibiting, and if so, what principles define this constraint. We employed a closed-loop intracortical brain-computer interface learning paradigm in which Rhesus macaques (Macaca mulatta) controlled a computer cursor by modulating neural activity patterns in the primary motor cortex. Using the brain-computer interface paradigm, we could specify and alter how neural activity mapped to cursor velocity. At the start of each session, we observed the characteristic activity patterns of the recorded neural population. The activity of a neural population can be represented in a high-dimensional space (termed the neural space), wherein each dimension corresponds to the activity of one neuron. These characteristic activity patterns comprise a low-dimensional subspace (termed the intrinsic manifold) within the neural space. The intrinsic manifold presumably reflects constraints imposed by the underlying neural circuitry. Here we show that the animals could readily learn to proficiently control the cursor using neural activity patterns that were within the intrinsic manifold. However, animals were less able to learn to proficiently control the cursor using activity patterns that were outside of the intrinsic manifold. These results suggest that the existing structure of a network can shape learning. On a timescale of hours, it seems to be difficult to learn to generate neural activity patterns that are not consistent with the existing network structure. These findings offer a network-level explanation for the observation that we are more readily able to learn new skills when they are related to the skills that we already possess.},
author = {Sadtler, Patrick T. and Quick, Kristin M. and Golub, Matthew D. and Chase, Steven M. and Ryu, Stephen I. and Tyler-Kabara, Elizabeth C. and Yu, Byron M. and Batista, Aaron P.},
doi = {10.1038/nature13665},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/neural manifold/neural{\_}constraints{\_}2014.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$n0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7515},
pages = {423--426},
pmid = {25164754},
publisher = {Nature Publishing Group},
title = {{Neural constraints on learning}},
url = {http://www.nature.com/doifinder/10.1038/nature13665},
volume = {512},
year = {2014}
}
@article{Brockett2014,
author = {Brockett, Roger},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/control{\_}theory/geo{\_}nonlinear{\_}control{\_}history.pdf:pdf},
journal = {Automatica},
pages = {2203--2224},
title = {{The early days of geometric nonlinear control}},
volume = {50},
year = {2014}
}
@article{Gao2015,
abstract = {Technological advances have dramatically expanded our ability to probe multi-neuronal dynamics and connectivity in the brain. However, our ability to extract a simple conceptual understanding from complex data is increasingly hampered by the lack of theoretically principled data analytic procedures, as well as theoretical frameworks for how circuit connectivity and dynamics can conspire to generate emergent behavioral and cognitive functions. We review and outline potential avenues for progress, including new theories of high dimensional data analysis, the need to analyze complex artificial networks, and methods for analyzing entire spaces of circuit models, rather than one model at a time. Such interplay between experiments, data analysis and theory will be indispensable in catalyzing conceptual advances in the age of large-scale neuroscience.},
author = {Gao, Peiran and Ganguli, Surya},
doi = {10.1016/j.conb.2015.04.003},
file = {:Users/spencerw/Dropbox (Personal)/UCL/literature/neural manifold/Gao{\_}CurrOpinion{\_}2015.pdf:pdf},
journal = {Curr. Opin. Neurobiol.},
pages = {148--155},
title = {{On simplicity and complexity in the brave new world of large-scale neuroscience This review comes from a themed issue on Large-scale recording technology}},
url = {http://dx.doi.org/10.1016/j.conb.2015.04.003},
volume = {32},
year = {2015}
}
@article{Betzel2016,
abstract = {To meet ongoing cognitive demands, the human brain must seamlessly transition from one brain state to another, in the process drawing on different cognitive systems. How does the brain's network of anatomical connections help facilitate such transitions? Which features of this network contribute to making one transition easy and another transition difficult? Here, we address these questions using network control theory. We calculate the optimal input signals to drive the brain to and from states dominated by different cognitive systems. The input signals allow us to assess the contributions made by different brain regions. We show that such contributions, which we measure as energy, are correlated with regions' weighted degrees. We also show that the network communicability, a measure of direct and indirect connectedness between brain regions, predicts the extent to which brain regions compensate when input to another region is suppressed. Finally, we identify optimal states in which the brain should start (and finish) in order to minimize transition energy. We show that the optimal target states display high activity in hub regions, implicating the brain's rich club. Furthermore, when rich club organization is destroyed, the energy cost associated with state transitions increases significantly, demonstrating that it is the richness of brain regions that makes them ideal targets.},
archivePrefix = {arXiv},
arxivId = {1603.05261},
author = {Betzel, Richard F. and Gu, Shi and Medaglia, John D. and Pasqualetti, Fabio and Bassett, Danielle S.},
doi = {10.1038/srep30770},
eprint = {1603.05261},
file = {:Users/spencerw/Library/Application Support/Mendeley Desktop/Downloaded/Betzel et al. - 2016 - Optimally controlling the human connectome the role of network topology.pdf:pdf},
issn = {2045-2322},
journal = {Nat. Publ. Gr.},
pages = {1--14},
pmid = {27468904},
publisher = {Nature Publishing Group},
title = {{Optimally controlling the human connectome: the role of network topology}},
year = {2016}
}
@article{Zanudo2016,
abstract = {What can we learn about controlling a system solely from its underlying network structure? Here we adapt a recently developed framework for control of networks governed by a broad class of nonlinear dynamics that includes the major dynamic models of biological, technological, and social processes. This feedback-based framework provides realizable node overrides that steer a system towards any of its natural long term dynamic behaviors, regardless of the specific functional forms and system parameters. We use this framework on several real networks, identify the topological characteristics that underlie the predicted node overrides, and compare its predictions to those of structural controllability in control theory. Finally, we demonstrate this framework's applicability in dynamic models of gene regulatory networks and identify nodes whose override is necessary for control in the general case, but not in specific model instances.},
archivePrefix = {arXiv},
arxivId = {1605.08415},
author = {Za{\~{n}}udo, Jorge G. T. and Yang, Gang and Albert, R{\'{e}}ka},
doi = {10.1073/pnas.1617387114},
eprint = {1605.08415},
file = {:Users/spencerw/Library/Application Support/Mendeley Desktop/Downloaded/Za{\~{n}}udo, Yang, Albert - 2016 - Structure-based control of complex networks with nonlinear dynamics.pdf:pdf},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci. U. S. A.},
number = {28},
pages = {1--33},
pmid = {28655847},
title = {{Structure-based control of complex networks with nonlinear dynamics}},
volume = {114},
year = {2016}
}
@article{Gao2017,
abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Dimensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
author = {Gao, Peiran and Trautmann, Eric and Yu, Byron M and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
journal = {bioRxiv},
month = {jan},
title = {{A theory of multineuronal dimensionality, dynamics and measurement}},
url = {http://biorxiv.org/content/early/2017/11/05/214262.abstract},
year = {2017}
}
@article{Tang2017,
abstract = {The ability to effectively control brain dynamics holds great promise for the enhancement of cognitive function in humans, and the betterment of their quality of life. Yet, successfully controlling dynamics in neural systems is challenging, in part due to the immense complexity of the brain and the large set of interactions that can drive any single change. While we have gained some understanding of the control of single neurons, the control of large-scale neural systems -- networks of multiply interacting components -- remains poorly understood. Efforts to address this gap include the construction of tools for the control of brain networks, mostly adapted from control and dynamical systems theory. Informed by current opportunities for practical intervention, these theoretical contributions provide models that draw from a wide array of mathematical approaches. We present intriguing recent developments for effective strategies of control in dynamic brain networks, and we also describe potential mechanisms that underlie such processes. We review efforts in the control of general neurophysiological processes with implications for brain development and cognitive function, as well as the control of altered neurophysiological processes in medical contexts such as anesthesia administration, seizure suppression, and deep-brain stimulation for Parkinson's disease. We conclude with a forward-looking discussion regarding how emerging results from network control -- especially approaches that deal with nonlinear dynamics or more realistic trajectories for control transitions -- could be used to directly address pressing questions in neuroscience.},
archivePrefix = {arXiv},
arxivId = {1701.01531},
author = {Tang, Evelyn and Bassett, Danielle S.},
eprint = {1701.01531},
file = {:Users/spencerw/Library/Application Support/Mendeley Desktop/Downloaded/Tang, Bassett - 2017 - Control of Dynamics in Brain Networks.pdf:pdf},
journal = {arXiv.org},
title = {{Control of Dynamics in Brain Networks}},
year = {2017}
}
