
\documentclass[a4paper,12pt]{article}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\small{Spencer Wilson}}
\chead{\small{SWC PhD Application}}
\rhead{\small{\today}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\begin{document}

HOW DO(ES THE BRAIN) (WE) CONTROL THE BRAIN ?? 

neural\_manifolds brief 
two paras with some math describing the theory 

para 1 -- general overview of the concept linking structure to dynamics 
para 2 -- the problem of formalizing nonlinear systems into a graph-like structure for analysis 
	of motifs, in some sort of matrix form
	perhaps the problem is our graph formalism
		does there exist another data structure that lends itself to this analysis 
	what information do we have?
		which cells are connected to which other cells and their strengths (time-dependent, slow)
		the potential of each cell as a time series (time-dependent, fast)

this is a theoretical project 

i think that there is something that hasn't been properly looked at w.r.t. the low dimensional manifolds and control theory-- how does the structure give rise to certain topologies that can be optimally controlled? do the dynamics support these ideas?

to-do
	look at spiking data 
	perform NLDR using manifold sculpting or something else (using Keras, scikit etc)
	code FVS algorithm 

connect structure to the dynamics -- can we choose nodes, or subnetworks of importance based on their contribution to the low-dimensional manifolds on which the dynamical trajectories lie? like finding a relationship between nodes that can change the manifold or can most effectively change the dynamics on that manifold? 
	some sort of dimensionality reduction and reconstruction from fewer nodes..? 
	can we score each node depending on its perturbative power...? 

read the brockett paper \cite{Brockett2014}
read structural control paper \cite{Zanudo2017}

\begin{align}
	dX_i/dt &= F_i(X_i,X_{I_i},t) \\ 
	dS_j/dt &= G_j(t)
\end{align}

if $N_s$ is the number of source nodes, then 

\begin{align}
&= \\ 
\end{align}

read the FVS papers \cite{Betzel} \cite{Whalen2016}

have to figure out how to write down the system in state space without sacrificing too much of the neural dynamics, but keeping things as smooth as possible... lots of balls in the air, captain. 

	there are certain system types (bilinear, etc) that lend themselves to the FVS analysis. we have to figure out what kinds of system allow for reasonable neural dynamics, and then what constraints are imposed on the manifolds of such systems. 


essentially:
	write down a simple model of neuron threshold dynamics 
	choose your favorite network model (degree distro, etc)
	find a connection between 
		the weight matrix of this graph 
		the threshold dynamics of individual neurons (are they all identical, is something missing in their dynamics (i.e. neurotransmitter effects?))
		the low-dimensional dynamics that come out

some people are using PCA to work out the relevant dimensions of the time series, then adding some of the nonlinearity back using the 4th order correlations (Barahona)

how do subspace manifolds inform the controllability of the system? 

% architecture => info processing => manifolds => identification of motifs, modules, and connections

% GOALS / IMPACT OF THE PROBLEM 

Thinking in control theoretic terms, can I identify structural subnetworks of neurons that contribute to the low-dimensional manifolds on which neural dynamical trajectories lie? Can I find neurons that can warp the manifold, the dynamics on that manifold, or both? Such a control theory for the brain will allow us to influence learning and decision making ``online,'' with targeted inputs. We can develop treatments in neuropathology, and we can think formally about psychopathology in a mathematical framework, where the manifold concept has been theorized for over a decade \cite{Gallese2003}. In developing a geometric neural control theory, I foresee potential applications to a range of neuroscientific interests including vision, sleep, and motor function. 


Indeed, interest has piqued recently concerning the phenomena of low-dimensional neural activity and its manifolds related to certain tasks \cite{Gao2015, Gao2017, Sadtler2014}. Similarly, there has been recent work applying the theories of optimal control to neuroscience \cite{Schiff2012}. However, my specific interest lies in understanding how stimuli that emanate externally from the environment and recurrently from the brain itself can be cast as control signals which modulate the (fast) dynamical processes of the brain as well as its (slow) learning processes. I aim to leverage the rapidly expanding cache of experimental data, both dynamic and connectomic, to understand how they are linked. By applying control theoretic techniques alongside tools from complex networks, I will work to relate the structure of neural circuitry to its macroscopic dynamics. 

\cite{Brockett2014} \cite{Tang2017, Zanudo2016}. 

I propose experiments \textit{in vivo} and \textit{in silico} simultaneously. In the former, we will use an optical brain-machine-interface to train a mouse on a simple task wherein chosen cells control neurofeedback. We would then image the cortical region implicated in the task and infer its structural connectivity using information theoretic techniques as well as its dynamical manifold through nonlinear dimensionality reduction \cite{Orlandi2014, Gashler2008}. Using metrics comprising graph theoretical measures from nonlinear systems theory combined with the geometry of dynamical manifolds, we predict which cells contribute most to the control cells' activities. This is the key theoretical step in this work. Finally, we ablate the predicted ``driver'' cells to effect a drop in performance as well as measure the compensation, if any, by the circuit to the change in structure. 

\textit{In silico}, we would generate neural data using a random graph model of a neural network according to our desired parameters. In this way, we can perform similar experiments numerically while forming hypotheses about how the macroscopic graph structure changes the control theoretic and geometric properties of the learned task. Rather than extracting structural information from imaging data, we design artificial neural networks to encode trajectories on nonlinear manifolds, and perform the same analyses for a range of parameters and generative models. With numerical experimentation we can support theoretical ideas between neural network structure and dynamics, develop new generative models, and compare our simulated hypotheses to experiments.   

\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}